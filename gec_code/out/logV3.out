GIT: b'add-bea-dataset\n' b'f19aa42d6063c2ebaf22df664293b4bf41692b2b\n'
2019-09-27 13:17:54
--------------------------------------------------------------------------------
Namespace(adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer', attention_dropout=0.2, bucket_cap_mb=25, clip_norm=2.0, copy_attention=True, copy_attention_dropout=0.2, copy_attention_heads=1, copy_ext_dict=False, cpu=False, criterion='cross_entropy', curriculum=0, data=['out/data_bin'], ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=512, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, ema_decay=0.9999, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, fix_batches_to_gpus=False, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, lazy_load=False, left_pad_source='True', left_pad_target='False', log_format=None, log_interval=1000, lr=[0.001], lr_period_updates=73328.0, lr_scheduler='triangular', lr_shrink=0.95, max_epoch=9, max_lr=0.004, max_sentences=64, max_sentences_valid=64, max_source_positions=1024, max_target_positions=1024, max_tokens=3000, max_update=0, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-05, momentum=0.99, no_ema=False, no_epoch_checkpoints=False, no_progress_bar=True, no_save=False, no_token_positional_embeddings=False, num_workers=0, optimizer='nag', optimizer_overrides='{}', positive_label_weight=1.2, pretrained_model='./out/models_pretrain/checkpoint9.pt', raw_text=False, relu_dropout=0.2, reset_lr_scheduler=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='out/modelsV3', save_interval=1, save_interval_updates=0, seed=4321, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, shrink_min=True, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, train_subset='train', update_freq=[1], upsample_primary=1, user_dir=None, valid_subset='valid,valid1', validate_interval=1, weight_decay=0.0)
| [src] dictionary: 49999 types
| [tgt] dictionary: 49999 types
Label file not found: out/data_bin/train.label.src.txt
Label file not found: out/data_bin/train.label.tgt.txt
| out/data_bin train 1168198 examples
Label file not found: out/data_bin/train1.label.src.txt
Label file not found: out/data_bin/train1.label.tgt.txt
| out/data_bin train1 34308 examples
Label file not found: out/data_bin/valid.label.src.txt
Label file not found: out/data_bin/valid.label.tgt.txt
| out/data_bin valid 4936 examples
Label file not found: out/data_bin/valid1.label.src.txt
Label file not found: out/data_bin/valid1.label.tgt.txt
| out/data_bin valid1 4384 examples
TransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(49999, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=512, bias=True)
        (layer_norms): ModuleList(
          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=512, bias=True)
        (layer_norms): ModuleList(
          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=512, bias=True)
        (layer_norms): ModuleList(
          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=512, bias=True)
        (layer_norms): ModuleList(
          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=512, bias=True)
        (layer_norms): ModuleList(
          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=512, bias=True)
        (layer_norms): ModuleList(
          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(49999, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (copy_attn_layer): MultiheadAttention(
      (out_proj): Linear(in_features=512, out_features=512, bias=True)
    )
    (copy_alpha_linear): Linear(in_features=512, out_features=1, bias=True)
  )
)
| model transformer, criterion CrossEntropyCriterion
| num. model params: 95979521 (num. trained: 95979521)
Load params from ./out/models_pretrain/checkpoint9.pt...
Load encoder.embed_tokens.weight...
Load encoder.layers.0.self_attn.in_proj_weight...
Load encoder.layers.0.self_attn.in_proj_bias...
Load encoder.layers.0.self_attn.out_proj.weight...
Load encoder.layers.0.self_attn.out_proj.bias...
Load encoder.layers.0.fc1.weight...
Load encoder.layers.0.fc1.bias...
Load encoder.layers.0.fc2.weight...
Load encoder.layers.0.fc2.bias...
Load encoder.layers.0.layer_norms.0.weight...
Load encoder.layers.0.layer_norms.0.bias...
Load encoder.layers.0.layer_norms.1.weight...
Load encoder.layers.0.layer_norms.1.bias...
Load encoder.layers.1.self_attn.in_proj_weight...
Load encoder.layers.1.self_attn.in_proj_bias...
Load encoder.layers.1.self_attn.out_proj.weight...
Load encoder.layers.1.self_attn.out_proj.bias...
Load encoder.layers.1.fc1.weight...
Load encoder.layers.1.fc1.bias...
Load encoder.layers.1.fc2.weight...
Load encoder.layers.1.fc2.bias...
Load encoder.layers.1.layer_norms.0.weight...
Load encoder.layers.1.layer_norms.0.bias...
Load encoder.layers.1.layer_norms.1.weight...
Load encoder.layers.1.layer_norms.1.bias...
Load encoder.layers.2.self_attn.in_proj_weight...
Load encoder.layers.2.self_attn.in_proj_bias...
Load encoder.layers.2.self_attn.out_proj.weight...
Load encoder.layers.2.self_attn.out_proj.bias...
Load encoder.layers.2.fc1.weight...
Load encoder.layers.2.fc1.bias...
Load encoder.layers.2.fc2.weight...
Load encoder.layers.2.fc2.bias...
Load encoder.layers.2.layer_norms.0.weight...
Load encoder.layers.2.layer_norms.0.bias...
Load encoder.layers.2.layer_norms.1.weight...
Load encoder.layers.2.layer_norms.1.bias...
Load encoder.layers.3.self_attn.in_proj_weight...
Load encoder.layers.3.self_attn.in_proj_bias...
Load encoder.layers.3.self_attn.out_proj.weight...
Load encoder.layers.3.self_attn.out_proj.bias...
Load encoder.layers.3.fc1.weight...
Load encoder.layers.3.fc1.bias...
Load encoder.layers.3.fc2.weight...
Load encoder.layers.3.fc2.bias...
Load encoder.layers.3.layer_norms.0.weight...
Load encoder.layers.3.layer_norms.0.bias...
Load encoder.layers.3.layer_norms.1.weight...
Load encoder.layers.3.layer_norms.1.bias...
Load encoder.layers.4.self_attn.in_proj_weight...
Load encoder.layers.4.self_attn.in_proj_bias...
Load encoder.layers.4.self_attn.out_proj.weight...
Load encoder.layers.4.self_attn.out_proj.bias...
Load encoder.layers.4.fc1.weight...
Load encoder.layers.4.fc1.bias...
Load encoder.layers.4.fc2.weight...
Load encoder.layers.4.fc2.bias...
Load encoder.layers.4.layer_norms.0.weight...
Load encoder.layers.4.layer_norms.0.bias...
Load encoder.layers.4.layer_norms.1.weight...
Load encoder.layers.4.layer_norms.1.bias...
Load encoder.layers.5.self_attn.in_proj_weight...
Load encoder.layers.5.self_attn.in_proj_bias...
Load encoder.layers.5.self_attn.out_proj.weight...
Load encoder.layers.5.self_attn.out_proj.bias...
Load encoder.layers.5.fc1.weight...
Load encoder.layers.5.fc1.bias...
Load encoder.layers.5.fc2.weight...
Load encoder.layers.5.fc2.bias...
Load encoder.layers.5.layer_norms.0.weight...
Load encoder.layers.5.layer_norms.0.bias...
Load encoder.layers.5.layer_norms.1.weight...
Load encoder.layers.5.layer_norms.1.bias...
Load decoder.layers.0.self_attn.in_proj_weight...
Load decoder.layers.0.self_attn.in_proj_bias...
Load decoder.layers.0.self_attn.out_proj.weight...
Load decoder.layers.0.self_attn.out_proj.bias...
Load decoder.layers.0.self_attn_layer_norm.weight...
Load decoder.layers.0.self_attn_layer_norm.bias...
Load decoder.layers.0.encoder_attn.in_proj_weight...
Load decoder.layers.0.encoder_attn.in_proj_bias...
Load decoder.layers.0.encoder_attn.out_proj.weight...
Load decoder.layers.0.encoder_attn.out_proj.bias...
Load decoder.layers.0.encoder_attn_layer_norm.weight...
Load decoder.layers.0.encoder_attn_layer_norm.bias...
Load decoder.layers.0.fc1.weight...
Load decoder.layers.0.fc1.bias...
Load decoder.layers.0.fc2.weight...
Load decoder.layers.0.fc2.bias...
Load decoder.layers.0.final_layer_norm.weight...
Load decoder.layers.0.final_layer_norm.bias...
Load decoder.layers.1.self_attn.in_proj_weight...
Load decoder.layers.1.self_attn.in_proj_bias...
Load decoder.layers.1.self_attn.out_proj.weight...
Load decoder.layers.1.self_attn.out_proj.bias...
Load decoder.layers.1.self_attn_layer_norm.weight...
Load decoder.layers.1.self_attn_layer_norm.bias...
Load decoder.layers.1.encoder_attn.in_proj_weight...
Load decoder.layers.1.encoder_attn.in_proj_bias...
Load decoder.layers.1.encoder_attn.out_proj.weight...
Load decoder.layers.1.encoder_attn.out_proj.bias...
Load decoder.layers.1.encoder_attn_layer_norm.weight...
Load decoder.layers.1.encoder_attn_layer_norm.bias...
Load decoder.layers.1.fc1.weight...
Load decoder.layers.1.fc1.bias...
Load decoder.layers.1.fc2.weight...
Load decoder.layers.1.fc2.bias...
Load decoder.layers.1.final_layer_norm.weight...
Load decoder.layers.1.final_layer_norm.bias...
Load decoder.layers.2.self_attn.in_proj_weight...
Load decoder.layers.2.self_attn.in_proj_bias...
Load decoder.layers.2.self_attn.out_proj.weight...
Load decoder.layers.2.self_attn.out_proj.bias...
Load decoder.layers.2.self_attn_layer_norm.weight...
Load decoder.layers.2.self_attn_layer_norm.bias...
Load decoder.layers.2.encoder_attn.in_proj_weight...
Load decoder.layers.2.encoder_attn.in_proj_bias...
Load decoder.layers.2.encoder_attn.out_proj.weight...
Load decoder.layers.2.encoder_attn.out_proj.bias...
Load decoder.layers.2.encoder_attn_layer_norm.weight...
Load decoder.layers.2.encoder_attn_layer_norm.bias...
Load decoder.layers.2.fc1.weight...
Load decoder.layers.2.fc1.bias...
Load decoder.layers.2.fc2.weight...
Load decoder.layers.2.fc2.bias...
Load decoder.layers.2.final_layer_norm.weight...
Load decoder.layers.2.final_layer_norm.bias...
Load decoder.layers.3.self_attn.in_proj_weight...
Load decoder.layers.3.self_attn.in_proj_bias...
Load decoder.layers.3.self_attn.out_proj.weight...
Load decoder.layers.3.self_attn.out_proj.bias...
Load decoder.layers.3.self_attn_layer_norm.weight...
Load decoder.layers.3.self_attn_layer_norm.bias...
Load decoder.layers.3.encoder_attn.in_proj_weight...
Load decoder.layers.3.encoder_attn.in_proj_bias...
Load decoder.layers.3.encoder_attn.out_proj.weight...
Load decoder.layers.3.encoder_attn.out_proj.bias...
Load decoder.layers.3.encoder_attn_layer_norm.weight...
Load decoder.layers.3.encoder_attn_layer_norm.bias...
Load decoder.layers.3.fc1.weight...
Load decoder.layers.3.fc1.bias...
Load decoder.layers.3.fc2.weight...
Load decoder.layers.3.fc2.bias...
Load decoder.layers.3.final_layer_norm.weight...
Load decoder.layers.3.final_layer_norm.bias...
Load decoder.layers.4.self_attn.in_proj_weight...
Load decoder.layers.4.self_attn.in_proj_bias...
Load decoder.layers.4.self_attn.out_proj.weight...
Load decoder.layers.4.self_attn.out_proj.bias...
Load decoder.layers.4.self_attn_layer_norm.weight...
Load decoder.layers.4.self_attn_layer_norm.bias...
Load decoder.layers.4.encoder_attn.in_proj_weight...
Load decoder.layers.4.encoder_attn.in_proj_bias...
Load decoder.layers.4.encoder_attn.out_proj.weight...
Load decoder.layers.4.encoder_attn.out_proj.bias...
Load decoder.layers.4.encoder_attn_layer_norm.weight...
Load decoder.layers.4.encoder_attn_layer_norm.bias...
Load decoder.layers.4.fc1.weight...
Load decoder.layers.4.fc1.bias...
Load decoder.layers.4.fc2.weight...
Load decoder.layers.4.fc2.bias...
Load decoder.layers.4.final_layer_norm.weight...
Load decoder.layers.4.final_layer_norm.bias...
Load decoder.layers.5.self_attn.in_proj_weight...
Load decoder.layers.5.self_attn.in_proj_bias...
Load decoder.layers.5.self_attn.out_proj.weight...
Load decoder.layers.5.self_attn.out_proj.bias...
Load decoder.layers.5.self_attn_layer_norm.weight...
Load decoder.layers.5.self_attn_layer_norm.bias...
Load decoder.layers.5.encoder_attn.in_proj_weight...
Load decoder.layers.5.encoder_attn.in_proj_bias...
Load decoder.layers.5.encoder_attn.out_proj.weight...
Load decoder.layers.5.encoder_attn.out_proj.bias...
Load decoder.layers.5.encoder_attn_layer_norm.weight...
Load decoder.layers.5.encoder_attn_layer_norm.bias...
Load decoder.layers.5.fc1.weight...
Load decoder.layers.5.fc1.bias...
Load decoder.layers.5.fc2.weight...
Load decoder.layers.5.fc2.bias...
Load decoder.layers.5.final_layer_norm.weight...
Load decoder.layers.5.final_layer_norm.bias...
Load decoder.copy_attn_layer.in_proj_weight...
Load decoder.copy_attn_layer.in_proj_bias...
Load decoder.copy_attn_layer.out_proj.weight...
Load decoder.copy_attn_layer.out_proj.bias...
Load decoder.copy_alpha_linear.weight...
Load decoder.copy_alpha_linear.bias...
| training on 1 GPUs
| max tokens per GPU = 3000 and max sentences per GPU = 64
| loaded checkpoint out/modelsV3/checkpoint_last.pt (epoch 2 @ 37752 updates)
| epoch 003:   1000 / 18876 loss=1.114, copy_alpha=0.677842, ppl=2.16, wps=6459, ups=6, wpb=1004.770, bsz=63.784, num_updates=38753, lr=0.00382907, gnorm=1.147, clip=0.021, oom=0.000, wall=176, train_wall=6236
| epoch 003:   2000 / 18876 loss=1.100, copy_alpha=0.676407, ppl=2.14, wps=6436, ups=6, wpb=1014.506, bsz=63.704, num_updates=39753, lr=0.00374725, gnorm=1.147, clip=0.021, oom=0.000, wall=336, train_wall=6389
| epoch 003:   3000 / 18876 loss=1.105, copy_alpha=0.676173, ppl=2.15, wps=6427, ups=6, wpb=1014.859, bsz=63.707, num_updates=40753, lr=0.00366542, gnorm=1.145, clip=0.023, oom=0.000, wall=495, train_wall=6540
| epoch 003:   4000 / 18876 loss=1.108, copy_alpha=0.675868, ppl=2.16, wps=6465, ups=6, wpb=1021.495, bsz=63.668, num_updates=41753, lr=0.0035836, gnorm=1.146, clip=0.022, oom=0.000, wall=653, train_wall=6692
| epoch 003:   5000 / 18876 loss=1.109, copy_alpha=0.675826, ppl=2.16, wps=6442, ups=6, wpb=1022.046, bsz=63.685, num_updates=42753, lr=0.00350177, gnorm=1.146, clip=0.023, oom=0.000, wall=814, train_wall=6846
| epoch 003:   6000 / 18876 loss=1.107, copy_alpha=0.675322, ppl=2.15, wps=6462, ups=6, wpb=1026.351, bsz=63.688, num_updates=43753, lr=0.00341995, gnorm=1.141, clip=0.022, oom=0.000, wall=974, train_wall=6999
| epoch 003:   7000 / 18876 loss=1.110, copy_alpha=0.675349, ppl=2.16, wps=6475, ups=6, wpb=1029.384, bsz=63.696, num_updates=44753, lr=0.00333812, gnorm=1.137, clip=0.021, oom=0.000, wall=1134, train_wall=7152
| epoch 003:   8000 / 18876 loss=1.110, copy_alpha=0.67545, ppl=2.16, wps=6480, ups=6, wpb=1030.685, bsz=63.709, num_updates=45753, lr=0.0032563, gnorm=1.135, clip=0.021, oom=0.000, wall=1293, train_wall=7305
| epoch 003:   9000 / 18876 loss=1.109, copy_alpha=0.675495, ppl=2.16, wps=6475, ups=6, wpb=1028.859, bsz=63.716, num_updates=46753, lr=0.00317448, gnorm=1.134, clip=0.021, oom=0.000, wall=1451, train_wall=7456
| epoch 003:  10000 / 18876 loss=1.105, copy_alpha=0.676229, ppl=2.15, wps=6475, ups=6, wpb=1029.281, bsz=63.704, num_updates=47753, lr=0.00309265, gnorm=1.132, clip=0.021, oom=0.000, wall=1610, train_wall=7609
| epoch 003:  11000 / 18876 loss=1.104, copy_alpha=0.675865, ppl=2.15, wps=6468, ups=6, wpb=1028.391, bsz=63.709, num_updates=48753, lr=0.00301083, gnorm=1.133, clip=0.022, oom=0.000, wall=1770, train_wall=7761
| epoch 003:  12000 / 18876 loss=1.104, copy_alpha=0.675743, ppl=2.15, wps=6479, ups=6, wpb=1031.228, bsz=63.710, num_updates=49753, lr=0.002929, gnorm=1.131, clip=0.021, oom=0.000, wall=1931, train_wall=7916
| epoch 003:  13000 / 18876 loss=1.104, copy_alpha=0.676091, ppl=2.15, wps=6484, ups=6, wpb=1031.952, bsz=63.708, num_updates=50753, lr=0.00284718, gnorm=1.130, clip=0.021, oom=0.000, wall=2090, train_wall=8069
| epoch 003:  14000 / 18876 loss=1.101, copy_alpha=0.675762, ppl=2.15, wps=6491, ups=6, wpb=1032.629, bsz=63.706, num_updates=51753, lr=0.00276536, gnorm=1.127, clip=0.021, oom=0.000, wall=2248, train_wall=8220
| epoch 003:  15000 / 18876 loss=1.101, copy_alpha=0.675784, ppl=2.14, wps=6505, ups=6, wpb=1035.539, bsz=63.709, num_updates=52753, lr=0.00268353, gnorm=1.124, clip=0.020, oom=0.000, wall=2409, train_wall=8375
| epoch 003:  16000 / 18876 loss=1.102, copy_alpha=0.676106, ppl=2.15, wps=6504, ups=6, wpb=1033.903, bsz=63.706, num_updates=53753, lr=0.00260171, gnorm=1.124, clip=0.021, oom=0.000, wall=2564, train_wall=8523
| epoch 003:  17000 / 18876 loss=1.102, copy_alpha=0.676484, ppl=2.15, wps=6499, ups=6, wpb=1033.653, bsz=63.701, num_updates=54753, lr=0.00251988, gnorm=1.123, clip=0.020, oom=0.000, wall=2725, train_wall=8677
| epoch 003:  18000 / 18876 loss=1.103, copy_alpha=0.676988, ppl=2.15, wps=6493, ups=6, wpb=1033.235, bsz=63.704, num_updates=55753, lr=0.00243806, gnorm=1.121, clip=0.020, oom=0.000, wall=2885, train_wall=8830
| epoch 003 | loss 1.103 | copy_alpha 0.677107 | ppl 2.15 | wps 6494 | ups 6 | wpb 1031.897 | bsz 63.706 | num_updates 56628 | lr 0.00236646 | gnorm 1.120 | clip 0.020 | oom 0.000 | wall 3020 | train_wall 8960
| epoch 003 | valid on 'valid' subset | loss 1.110 | ppl 2.16 | num_updates 56628 | best_loss 1.11046 | copy_alpha 0.798789
| epoch 003 | valid on 'valid1' subset | loss 0.511 | ppl 1.42 | num_updates 56628 | best_loss 0.510827 | copy_alpha 0.74487
| epoch 003 | valid on 'valid' subset | loss 1.098 | ppl 2.14 | num_updates 56628 | best_loss 1.09828 | copy_alpha 0.805984
| epoch 003 | valid on 'valid1' subset | loss 0.503 | ppl 1.42 | num_updates 56628 | best_loss 0.50337 | copy_alpha 0.757718
| epoch 004:   1000 / 18876 loss=1.050, copy_alpha=0.683205, ppl=2.07, wps=6547, ups=5, wpb=1029.729, bsz=63.680, num_updates=57629, lr=0.00228456, gnorm=1.095, clip=0.014, oom=0.000, wall=3208, train_wall=9111
| epoch 004:   2000 / 18876 loss=1.045, copy_alpha=0.683607, ppl=2.06, wps=6467, ups=6, wpb=1030.777, bsz=63.692, num_updates=58629, lr=0.00220273, gnorm=1.098, clip=0.018, oom=0.000, wall=3369, train_wall=9265
| epoch 004:   3000 / 18876 loss=1.046, copy_alpha=0.683452, ppl=2.06, wps=6477, ups=6, wpb=1030.078, bsz=63.691, num_updates=59629, lr=0.00212091, gnorm=1.100, clip=0.017, oom=0.000, wall=3527, train_wall=9417
| epoch 004:   4000 / 18876 loss=1.038, copy_alpha=0.682196, ppl=2.05, wps=6493, ups=6, wpb=1030.553, bsz=63.722, num_updates=60629, lr=0.00203908, gnorm=1.098, clip=0.016, oom=0.000, wall=3685, train_wall=9568
| epoch 004:   5000 / 18876 loss=1.035, copy_alpha=0.68242, ppl=2.05, wps=6500, ups=6, wpb=1030.079, bsz=63.734, num_updates=61629, lr=0.00195726, gnorm=1.098, clip=0.017, oom=0.000, wall=3843, train_wall=9719
| epoch 004:   6000 / 18876 loss=1.041, copy_alpha=0.683907, ppl=2.06, wps=6486, ups=6, wpb=1029.724, bsz=63.724, num_updates=62629, lr=0.00187544, gnorm=1.099, clip=0.018, oom=0.000, wall=4003, train_wall=9873
| epoch 004:   7000 / 18876 loss=1.043, copy_alpha=0.684023, ppl=2.06, wps=6484, ups=6, wpb=1028.903, bsz=63.735, num_updates=63629, lr=0.00179361, gnorm=1.101, clip=0.019, oom=0.000, wall=4161, train_wall=10024
| epoch 004:   8000 / 18876 loss=1.040, copy_alpha=0.683787, ppl=2.06, wps=6488, ups=6, wpb=1029.905, bsz=63.707, num_updates=64629, lr=0.00171179, gnorm=1.099, clip=0.019, oom=0.000, wall=4320, train_wall=10176
| epoch 004:   9000 / 18876 loss=1.040, copy_alpha=0.683955, ppl=2.06, wps=6495, ups=6, wpb=1029.732, bsz=63.717, num_updates=65629, lr=0.00162996, gnorm=1.098, clip=0.018, oom=0.000, wall=4477, train_wall=10327
| epoch 004:  10000 / 18876 loss=1.041, copy_alpha=0.68399, ppl=2.06, wps=6492, ups=6, wpb=1030.025, bsz=63.720, num_updates=66629, lr=0.00154814, gnorm=1.098, clip=0.018, oom=0.000, wall=4637, train_wall=10480
| epoch 004:  11000 / 18876 loss=1.039, copy_alpha=0.683905, ppl=2.05, wps=6488, ups=6, wpb=1030.755, bsz=63.723, num_updates=67629, lr=0.00146632, gnorm=1.095, clip=0.017, oom=0.000, wall=4798, train_wall=10634
| epoch 004:  12000 / 18876 loss=1.039, copy_alpha=0.68409, ppl=2.06, wps=6492, ups=6, wpb=1031.831, bsz=63.714, num_updates=68629, lr=0.00138449, gnorm=1.094, clip=0.017, oom=0.000, wall=4958, train_wall=10787
| epoch 004:  13000 / 18876 loss=1.040, copy_alpha=0.68373, ppl=2.06, wps=6497, ups=6, wpb=1034.507, bsz=63.708, num_updates=69629, lr=0.00130267, gnorm=1.093, clip=0.016, oom=0.000, wall=5120, train_wall=10943
| epoch 004:  14000 / 18876 loss=1.039, copy_alpha=0.684053, ppl=2.06, wps=6492, ups=6, wpb=1033.427, bsz=63.694, num_updates=70629, lr=0.00122084, gnorm=1.093, clip=0.016, oom=0.000, wall=5279, train_wall=11095
| epoch 004:  15000 / 18876 loss=1.040, copy_alpha=0.684073, ppl=2.06, wps=6488, ups=6, wpb=1031.478, bsz=63.700, num_updates=71629, lr=0.00113902, gnorm=1.093, clip=0.016, oom=0.000, wall=5435, train_wall=11244
| epoch 004:  16000 / 18876 loss=1.038, copy_alpha=0.684069, ppl=2.05, wps=6487, ups=6, wpb=1031.031, bsz=63.710, num_updates=72629, lr=0.0010572, gnorm=1.091, clip=0.016, oom=0.000, wall=5593, train_wall=11396
| epoch 004:  17000 / 18876 loss=1.036, copy_alpha=0.683963, ppl=2.05, wps=6486, ups=6, wpb=1031.830, bsz=63.711, num_updates=73629, lr=0.000973398, gnorm=1.090, clip=0.015, oom=0.000, wall=5755, train_wall=11550
| epoch 004:  18000 / 18876 loss=1.035, copy_alpha=0.683977, ppl=2.05, wps=6486, ups=6, wpb=1031.553, bsz=63.706, num_updates=74629, lr=0.00105113, gnorm=1.089, clip=0.015, oom=0.000, wall=5913, train_wall=11702
| epoch 004 | loss 1.035 | copy_alpha 0.683825 | ppl 2.05 | wps 6487 | ups 6 | wpb 1031.897 | bsz 63.706 | num_updates 75504 | lr 0.00111915 | gnorm 1.088 | clip 0.015 | oom 0.000 | wall 6053 | train_wall 11836
| epoch 004 | valid on 'valid' subset | loss 1.080 | ppl 2.11 | num_updates 75504 | best_loss 1.08003 | copy_alpha 0.818997
| epoch 004 | valid on 'valid1' subset | loss 0.497 | ppl 1.41 | num_updates 75504 | best_loss 0.496556 | copy_alpha 0.773717
| epoch 004 | valid on 'valid' subset | loss 1.079 | ppl 2.11 | num_updates 75504 | best_loss 1.07887 | copy_alpha 0.820776
| epoch 004 | valid on 'valid1' subset | loss 0.495 | ppl 1.41 | num_updates 75504 | best_loss 0.495494 | copy_alpha 0.773947
| epoch 005:   1000 / 18876 loss=1.003, copy_alpha=0.688106, ppl=2.00, wps=6567, ups=5, wpb=1038.834, bsz=63.768, num_updates=76505, lr=0.00119696, gnorm=1.070, clip=0.018, oom=0.000, wall=6240, train_wall=11988
| epoch 005:   2000 / 18876 loss=1.002, copy_alpha=0.688316, ppl=2.00, wps=6557, ups=6, wpb=1033.551, bsz=63.681, num_updates=77505, lr=0.00127469, gnorm=1.083, clip=0.018, oom=0.000, wall=6398, train_wall=12138
| epoch 005:   3000 / 18876 loss=1.006, copy_alpha=0.688637, ppl=2.01, wps=6526, ups=6, wpb=1032.521, bsz=63.699, num_updates=78505, lr=0.00135242, gnorm=1.087, clip=0.014, oom=0.000, wall=6557, train_wall=12291
| epoch 005:   4000 / 18876 loss=1.003, copy_alpha=0.687783, ppl=2.00, wps=6533, ups=6, wpb=1030.786, bsz=63.707, num_updates=79505, lr=0.00143016, gnorm=1.089, clip=0.017, oom=0.000, wall=6713, train_wall=12441
| epoch 005:   5000 / 18876 loss=1.006, copy_alpha=0.688233, ppl=2.01, wps=6528, ups=6, wpb=1030.966, bsz=63.719, num_updates=80505, lr=0.00150789, gnorm=1.090, clip=0.017, oom=0.000, wall=6872, train_wall=12593
| epoch 005:   6000 / 18876 loss=1.008, copy_alpha=0.688639, ppl=2.01, wps=6490, ups=6, wpb=1025.477, bsz=63.696, num_updates=81505, lr=0.00158562, gnorm=1.094, clip=0.017, oom=0.000, wall=7030, train_wall=12744
| epoch 005:   7000 / 18876 loss=1.006, copy_alpha=0.68828, ppl=2.01, wps=6491, ups=6, wpb=1027.703, bsz=63.707, num_updates=82505, lr=0.00166336, gnorm=1.095, clip=0.017, oom=0.000, wall=7191, train_wall=12898
| epoch 005:   8000 / 18876 loss=1.006, copy_alpha=0.687817, ppl=2.01, wps=6487, ups=6, wpb=1027.927, bsz=63.711, num_updates=83505, lr=0.00174109, gnorm=1.096, clip=0.017, oom=0.000, wall=7350, train_wall=13050
| epoch 005:   9000 / 18876 loss=1.009, copy_alpha=0.688272, ppl=2.01, wps=6478, ups=6, wpb=1024.374, bsz=63.718, num_updates=84505, lr=0.00181882, gnorm=1.099, clip=0.017, oom=0.000, wall=7506, train_wall=13199
| epoch 005:  10000 / 18876 loss=1.009, copy_alpha=0.688235, ppl=2.01, wps=6474, ups=6, wpb=1025.361, bsz=63.720, num_updates=85505, lr=0.00189655, gnorm=1.099, clip=0.017, oom=0.000, wall=7666, train_wall=13352
| epoch 005:  11000 / 18876 loss=1.008, copy_alpha=0.687967, ppl=2.01, wps=6464, ups=6, wpb=1025.843, bsz=63.729, num_updates=86505, lr=0.00197429, gnorm=1.098, clip=0.017, oom=0.000, wall=7828, train_wall=13507
| epoch 005:  12000 / 18876 loss=1.005, copy_alpha=0.687209, ppl=2.01, wps=6461, ups=6, wpb=1027.724, bsz=63.727, num_updates=87505, lr=0.00205202, gnorm=1.097, clip=0.017, oom=0.000, wall=7991, train_wall=13663
| epoch 005:  13000 / 18876 loss=1.006, copy_alpha=0.686793, ppl=2.01, wps=6457, ups=6, wpb=1028.924, bsz=63.720, num_updates=88505, lr=0.00212975, gnorm=1.097, clip=0.017, oom=0.000, wall=8154, train_wall=13818
| epoch 005:  14000 / 18876 loss=1.009, copy_alpha=0.686627, ppl=2.01, wps=6457, ups=6, wpb=1030.522, bsz=63.714, num_updates=89505, lr=0.00220749, gnorm=1.097, clip=0.017, oom=0.000, wall=8317, train_wall=13973
| epoch 005:  15000 / 18876 loss=1.011, copy_alpha=0.68637, ppl=2.01, wps=6453, ups=6, wpb=1029.944, bsz=63.716, num_updates=90505, lr=0.00228522, gnorm=1.099, clip=0.017, oom=0.000, wall=8477, train_wall=14126
| epoch 005:  16000 / 18876 loss=1.009, copy_alpha=0.686178, ppl=2.01, wps=6445, ups=6, wpb=1029.353, bsz=63.720, num_updates=91505, lr=0.00236295, gnorm=1.099, clip=0.017, oom=0.000, wall=8638, train_wall=14280
| epoch 005:  17000 / 18876 loss=1.012, copy_alpha=0.68591, ppl=2.02, wps=6441, ups=6, wpb=1031.062, bsz=63.711, num_updates=92505, lr=0.00244068, gnorm=1.100, clip=0.017, oom=0.000, wall=8804, train_wall=14438
| epoch 005:  18000 / 18876 loss=1.013, copy_alpha=0.685493, ppl=2.02, wps=6442, ups=6, wpb=1031.333, bsz=63.703, num_updates=93505, lr=0.00251842, gnorm=1.100, clip=0.017, oom=0.000, wall=8964, train_wall=14592
| epoch 005 | loss 1.014 | copy_alpha 0.685608 | ppl 2.02 | wps 6443 | ups 6 | wpb 1031.897 | bsz 63.706 | num_updates 94380 | lr 0.00258643 | gnorm 1.102 | clip 0.017 | oom 0.000 | wall 9105 | train_wall 14726
| epoch 005 | valid on 'valid' subset | loss 1.092 | ppl 2.13 | num_updates 94380 | best_loss 1.08003 | copy_alpha 0.844345
| epoch 005 | valid on 'valid1' subset | loss 0.506 | ppl 1.42 | num_updates 94380 | best_loss 0.505944 | copy_alpha 0.799226
| epoch 005 | valid on 'valid' subset | loss 1.072 | ppl 2.10 | num_updates 94380 | best_loss 1.07175 | copy_alpha 0.821196
| epoch 005 | valid on 'valid1' subset | loss 0.491 | ppl 1.41 | num_updates 94380 | best_loss 0.49121 | copy_alpha 0.773636
| epoch 006:   1000 / 18876 loss=1.003, copy_alpha=0.69043, ppl=2.00, wps=6488, ups=5, wpb=1031.413, bsz=63.624, num_updates=95381, lr=0.00266424, gnorm=1.115, clip=0.019, oom=0.000, wall=9292, train_wall=14878
| epoch 006:   2000 / 18876 loss=1.009, copy_alpha=0.688415, ppl=2.01, wps=6419, ups=6, wpb=1028.907, bsz=63.704, num_updates=96381, lr=0.00274198, gnorm=1.125, clip=0.020, oom=0.000, wall=9454, train_wall=15032
| epoch 006:   3000 / 18876 loss=1.022, copy_alpha=0.687928, ppl=2.03, wps=6426, ups=6, wpb=1027.463, bsz=63.685, num_updates=97381, lr=0.00281971, gnorm=1.133, clip=0.022, oom=0.000, wall=9613, train_wall=15185
| epoch 006:   4000 / 18876 loss=1.020, copy_alpha=0.685595, ppl=2.03, wps=6459, ups=6, wpb=1030.335, bsz=63.686, num_updates=98381, lr=0.00289744, gnorm=1.133, clip=0.020, oom=0.000, wall=9772, train_wall=15336
| epoch 006:   5000 / 18876 loss=1.020, copy_alpha=0.684819, ppl=2.03, wps=6441, ups=6, wpb=1028.540, bsz=63.694, num_updates=99381, lr=0.00297518, gnorm=1.135, clip=0.021, oom=0.000, wall=9932, train_wall=15489
| epoch 006:   6000 / 18876 loss=1.017, copy_alpha=0.685018, ppl=2.02, wps=6446, ups=6, wpb=1026.458, bsz=63.719, num_updates=100381, lr=0.00305291, gnorm=1.136, clip=0.021, oom=0.000, wall=10089, train_wall=15639
| epoch 006:   7000 / 18876 loss=1.020, copy_alpha=0.68465, ppl=2.03, wps=6442, ups=6, wpb=1025.234, bsz=63.694, num_updates=101381, lr=0.00313064, gnorm=1.137, clip=0.022, oom=0.000, wall=10248, train_wall=15791
| epoch 006:   8000 / 18876 loss=1.021, copy_alpha=0.684374, ppl=2.03, wps=6439, ups=6, wpb=1024.190, bsz=63.700, num_updates=102381, lr=0.00320837, gnorm=1.138, clip=0.022, oom=0.000, wall=10406, train_wall=15943
| epoch 006:   9000 / 18876 loss=1.024, copy_alpha=0.684257, ppl=2.03, wps=6431, ups=6, wpb=1024.631, bsz=63.684, num_updates=103381, lr=0.00328611, gnorm=1.140, clip=0.023, oom=0.000, wall=10567, train_wall=16097
| epoch 006:  10000 / 18876 loss=1.024, copy_alpha=0.683924, ppl=2.03, wps=6446, ups=6, wpb=1027.934, bsz=63.692, num_updates=104381, lr=0.00336384, gnorm=1.137, clip=0.021, oom=0.000, wall=10728, train_wall=16251
| epoch 006:  11000 / 18876 loss=1.024, copy_alpha=0.683669, ppl=2.03, wps=6443, ups=6, wpb=1027.484, bsz=63.697, num_updates=105381, lr=0.00344157, gnorm=1.138, clip=0.021, oom=0.000, wall=10888, train_wall=16403
| epoch 006:  12000 / 18876 loss=1.028, copy_alpha=0.68368, ppl=2.04, wps=6439, ups=6, wpb=1026.973, bsz=63.705, num_updates=106381, lr=0.00351931, gnorm=1.139, clip=0.021, oom=0.000, wall=11047, train_wall=16555
| epoch 006:  13000 / 18876 loss=1.027, copy_alpha=0.683534, ppl=2.04, wps=6432, ups=6, wpb=1026.974, bsz=63.701, num_updates=107381, lr=0.00359704, gnorm=1.146, clip=0.021, oom=0.000, wall=11209, train_wall=16710
| epoch 006:  14000 / 18876 loss=1.029, copy_alpha=0.683063, ppl=2.04, wps=6425, ups=6, wpb=1027.402, bsz=63.694, num_updates=108381, lr=0.00367477, gnorm=1.147, clip=0.021, oom=0.000, wall=11372, train_wall=16865
| epoch 006:  15000 / 18876 loss=1.030, copy_alpha=0.68266, ppl=2.04, wps=6419, ups=6, wpb=1027.283, bsz=63.698, num_updates=109381, lr=0.00375251, gnorm=1.152, clip=0.022, oom=0.000, wall=11534, train_wall=17019
| epoch 006:  16000 / 18876 loss=1.031, copy_alpha=0.682571, ppl=2.04, wps=6423, ups=6, wpb=1027.989, bsz=63.706, num_updates=110381, lr=0.00376976, gnorm=1.151, clip=0.022, oom=0.000, wall=11694, train_wall=17173
| epoch 006:  17000 / 18876 loss=1.031, copy_alpha=0.681941, ppl=2.04, wps=6425, ups=6, wpb=1029.601, bsz=63.704, num_updates=111381, lr=0.00369203, gnorm=1.149, clip=0.022, oom=0.000, wall=11858, train_wall=17329
| epoch 006:  18000 / 18876 loss=1.033, copy_alpha=0.681578, ppl=2.05, wps=6432, ups=6, wpb=1031.271, bsz=63.708, num_updates=112381, lr=0.0036143, gnorm=1.148, clip=0.022, oom=0.000, wall=12020, train_wall=17484
| epoch 006 | loss 1.033 | copy_alpha 0.681377 | ppl 2.05 | wps 6435 | ups 6 | wpb 1031.897 | bsz 63.706 | num_updates 113256 | lr 0.00354628 | gnorm 1.147 | clip 0.022 | oom 0.000 | wall 12160 | train_wall 17618
| epoch 006 | valid on 'valid' subset | loss 1.109 | ppl 2.16 | num_updates 113256 | best_loss 1.08003 | copy_alpha 0.817037
| epoch 006 | valid on 'valid1' subset | loss 0.508 | ppl 1.42 | num_updates 113256 | best_loss 0.508318 | copy_alpha 0.773027
| epoch 006 | valid on 'valid' subset | loss 1.072 | ppl 2.10 | num_updates 113256 | best_loss 1.07211 | copy_alpha 0.811109
| epoch 006 | valid on 'valid1' subset | loss 0.490 | ppl 1.40 | num_updates 113256 | best_loss 0.490264 | copy_alpha 0.762792
| epoch 007:   1000 / 18876 loss=0.983, copy_alpha=0.677835, ppl=1.98, wps=6488, ups=5, wpb=1058.512, bsz=63.664, num_updates=114257, lr=0.00346847, gnorm=1.116, clip=0.014, oom=0.000, wall=12351, train_wall=17774
| epoch 007:   2000 / 18876 loss=1.004, copy_alpha=0.684128, ppl=2.01, wps=6435, ups=6, wpb=1025.757, bsz=63.728, num_updates=115257, lr=0.00339074, gnorm=1.144, clip=0.021, oom=0.000, wall=12507, train_wall=17923
| epoch 007:   3000 / 18876 loss=1.009, copy_alpha=0.68272, ppl=2.01, wps=6423, ups=6, wpb=1032.941, bsz=63.697, num_updates=116257, lr=0.003313, gnorm=1.147, clip=0.022, oom=0.000, wall=12671, train_wall=18079
| epoch 007:   4000 / 18876 loss=1.012, copy_alpha=0.682927, ppl=2.02, wps=6424, ups=6, wpb=1033.383, bsz=63.691, num_updates=117257, lr=0.00323527, gnorm=1.148, clip=0.023, oom=0.000, wall=12832, train_wall=18233
| epoch 007:   5000 / 18876 loss=1.009, copy_alpha=0.682705, ppl=2.01, wps=6427, ups=6, wpb=1033.529, bsz=63.682, num_updates=118257, lr=0.00315754, gnorm=1.146, clip=0.022, oom=0.000, wall=12992, train_wall=18387
| epoch 007:   6000 / 18876 loss=1.010, copy_alpha=0.683127, ppl=2.01, wps=6407, ups=6, wpb=1032.462, bsz=63.700, num_updates=119257, lr=0.0030798, gnorm=1.146, clip=0.022, oom=0.000, wall=13155, train_wall=18542
| epoch 007:   7000 / 18876 loss=1.014, copy_alpha=0.683516, ppl=2.02, wps=6406, ups=6, wpb=1028.817, bsz=63.704, num_updates=120257, lr=0.00300207, gnorm=1.146, clip=0.022, oom=0.000, wall=13313, train_wall=18692
| epoch 007:   8000 / 18876 loss=1.012, copy_alpha=0.682508, ppl=2.02, wps=6422, ups=6, wpb=1032.819, bsz=63.677, num_updates=121257, lr=0.00292434, gnorm=1.145, clip=0.022, oom=0.000, wall=13475, train_wall=18847
| epoch 007:   9000 / 18876 loss=1.009, copy_alpha=0.682091, ppl=2.01, wps=6417, ups=6, wpb=1033.317, bsz=63.688, num_updates=122257, lr=0.00284661, gnorm=1.143, clip=0.022, oom=0.000, wall=13637, train_wall=19003
| epoch 007:  10000 / 18876 loss=1.010, copy_alpha=0.681974, ppl=2.01, wps=6420, ups=6, wpb=1033.111, bsz=63.694, num_updates=123257, lr=0.00276887, gnorm=1.144, clip=0.022, oom=0.000, wall=13797, train_wall=19156
| epoch 007:  11000 / 18876 loss=1.009, copy_alpha=0.681947, ppl=2.01, wps=6418, ups=6, wpb=1034.400, bsz=63.705, num_updates=124257, lr=0.00269114, gnorm=1.141, clip=0.022, oom=0.000, wall=13961, train_wall=19312
| epoch 007:  12000 / 18876 loss=1.011, copy_alpha=0.682334, ppl=2.02, wps=6424, ups=6, wpb=1034.341, bsz=63.705, num_updates=125257, lr=0.00261341, gnorm=1.140, clip=0.022, oom=0.000, wall=14120, train_wall=19464
| epoch 007:  13000 / 18876 loss=1.010, copy_alpha=0.68248, ppl=2.01, wps=6425, ups=6, wpb=1033.976, bsz=63.709, num_updates=126257, lr=0.00253567, gnorm=1.138, clip=0.022, oom=0.000, wall=14280, train_wall=19617
| epoch 007:  14000 / 18876 loss=1.010, copy_alpha=0.682506, ppl=2.01, wps=6427, ups=6, wpb=1034.374, bsz=63.696, num_updates=127257, lr=0.00245794, gnorm=1.137, clip=0.022, oom=0.000, wall=14442, train_wall=19771
| epoch 007:  15000 / 18876 loss=1.010, copy_alpha=0.682386, ppl=2.01, wps=6420, ups=6, wpb=1034.208, bsz=63.693, num_updates=128257, lr=0.00238021, gnorm=1.136, clip=0.022, oom=0.000, wall=14604, train_wall=19927
| epoch 007:  16000 / 18876 loss=1.009, copy_alpha=0.682204, ppl=2.01, wps=6422, ups=6, wpb=1033.586, bsz=63.692, num_updates=129257, lr=0.00230248, gnorm=1.135, clip=0.022, oom=0.000, wall=14763, train_wall=20079
| epoch 007:  17000 / 18876 loss=1.010, copy_alpha=0.682595, ppl=2.01, wps=6419, ups=6, wpb=1032.481, bsz=63.700, num_updates=130257, lr=0.00222474, gnorm=1.134, clip=0.021, oom=0.000, wall=14923, train_wall=20231
| epoch 007:  18000 / 18876 loss=1.011, copy_alpha=0.68295, ppl=2.01, wps=6413, ups=6, wpb=1032.062, bsz=63.705, num_updates=131257, lr=0.00214701, gnorm=1.134, clip=0.021, oom=0.000, wall=15085, train_wall=20386
| epoch 007 | loss 1.010 | copy_alpha 0.68303 | ppl 2.01 | wps 6414 | ups 6 | wpb 1031.897 | bsz 63.706 | num_updates 132132 | lr 0.00207899 | gnorm 1.133 | clip 0.021 | oom 0.000 | wall 15225 | train_wall 20519
| epoch 007 | valid on 'valid' subset | loss 1.076 | ppl 2.11 | num_updates 132132 | best_loss 1.07601 | copy_alpha 0.837609
| epoch 007 | valid on 'valid1' subset | loss 0.506 | ppl 1.42 | num_updates 132132 | best_loss 0.505622 | copy_alpha 0.794678
| epoch 007 | valid on 'valid' subset | loss 1.066 | ppl 2.09 | num_updates 132132 | best_loss 1.06596 | copy_alpha 0.816974
| epoch 007 | valid on 'valid1' subset | loss 0.490 | ppl 1.40 | num_updates 132132 | best_loss 0.49042 | copy_alpha 0.769711
| epoch 008:   1000 / 18876 loss=0.951, copy_alpha=0.69429, ppl=1.93, wps=6510, ups=5, wpb=1045.089, bsz=63.736, num_updates=133133, lr=0.00200118, gnorm=1.106, clip=0.014, oom=0.000, wall=15420, train_wall=20673
| epoch 008:   2000 / 18876 loss=0.955, copy_alpha=0.692051, ppl=1.94, wps=6469, ups=6, wpb=1033.347, bsz=63.664, num_updates=134133, lr=0.00192345, gnorm=1.115, clip=0.015, oom=0.000, wall=15579, train_wall=20825
| epoch 008:   3000 / 18876 loss=0.952, copy_alpha=0.691473, ppl=1.93, wps=6464, ups=6, wpb=1034.707, bsz=63.637, num_updates=135133, lr=0.00184572, gnorm=1.115, clip=0.016, oom=0.000, wall=15740, train_wall=20978
| epoch 008:   4000 / 18876 loss=0.950, copy_alpha=0.691264, ppl=1.93, wps=6450, ups=6, wpb=1028.130, bsz=63.660, num_updates=136133, lr=0.00176798, gnorm=1.117, clip=0.016, oom=0.000, wall=15897, train_wall=21129
| epoch 008:   5000 / 18876 loss=0.950, copy_alpha=0.690512, ppl=1.93, wps=6447, ups=6, wpb=1027.801, bsz=63.685, num_updates=137133, lr=0.00169025, gnorm=1.120, clip=0.018, oom=0.000, wall=16057, train_wall=21282
| epoch 008:   6000 / 18876 loss=0.956, copy_alpha=0.690364, ppl=1.94, wps=6434, ups=6, wpb=1026.559, bsz=63.677, num_updates=138133, lr=0.00161252, gnorm=1.121, clip=0.018, oom=0.000, wall=16217, train_wall=21435
| epoch 008:   7000 / 18876 loss=0.957, copy_alpha=0.691396, ppl=1.94, wps=6438, ups=6, wpb=1025.386, bsz=63.703, num_updates=139133, lr=0.00153478, gnorm=1.123, clip=0.018, oom=0.000, wall=16374, train_wall=21586
| epoch 008:   8000 / 18876 loss=0.955, copy_alpha=0.690648, ppl=1.94, wps=6452, ups=6, wpb=1026.227, bsz=63.702, num_updates=140133, lr=0.00145705, gnorm=1.121, clip=0.018, oom=0.000, wall=16532, train_wall=21737
| epoch 008:   9000 / 18876 loss=0.957, copy_alpha=0.690573, ppl=1.94, wps=6472, ups=6, wpb=1027.478, bsz=63.700, num_updates=141133, lr=0.00137932, gnorm=1.120, clip=0.017, oom=0.000, wall=16688, train_wall=21887
| epoch 008:  10000 / 18876 loss=0.958, copy_alpha=0.690256, ppl=1.94, wps=6479, ups=6, wpb=1029.040, bsz=63.685, num_updates=142133, lr=0.00130159, gnorm=1.118, clip=0.017, oom=0.000, wall=16848, train_wall=22040
| epoch 008:  11000 / 18876 loss=0.957, copy_alpha=0.689977, ppl=1.94, wps=6491, ups=6, wpb=1029.419, bsz=63.702, num_updates=143133, lr=0.00122385, gnorm=1.117, clip=0.017, oom=0.000, wall=17004, train_wall=22190
| epoch 008:  12000 / 18876 loss=0.955, copy_alpha=0.689487, ppl=1.94, wps=6507, ups=6, wpb=1034.160, bsz=63.696, num_updates=144133, lr=0.00114612, gnorm=1.114, clip=0.017, oom=0.000, wall=17167, train_wall=22346
| epoch 008:  13000 / 18876 loss=0.956, copy_alpha=0.689519, ppl=1.94, wps=6505, ups=6, wpb=1033.504, bsz=63.705, num_updates=145133, lr=0.00106839, gnorm=1.114, clip=0.017, oom=0.000, wall=17325, train_wall=22497
| epoch 008:  14000 / 18876 loss=0.957, copy_alpha=0.690085, ppl=1.94, wps=6506, ups=6, wpb=1032.131, bsz=63.709, num_updates=146133, lr=0.000990654, gnorm=1.114, clip=0.018, oom=0.000, wall=17481, train_wall=22646
| epoch 008:  15000 / 18876 loss=0.959, copy_alpha=0.690222, ppl=1.94, wps=6505, ups=6, wpb=1031.881, bsz=63.715, num_updates=147133, lr=0.000937725, gnorm=1.114, clip=0.017, oom=0.000, wall=17639, train_wall=22798
| epoch 008:  16000 / 18876 loss=0.959, copy_alpha=0.69027, ppl=1.94, wps=6509, ups=6, wpb=1031.551, bsz=63.711, num_updates=148133, lr=0.00101157, gnorm=1.114, clip=0.017, oom=0.000, wall=17795, train_wall=22948
| epoch 008:  17000 / 18876 loss=0.959, copy_alpha=0.690298, ppl=1.94, wps=6513, ups=6, wpb=1031.770, bsz=63.720, num_updates=149133, lr=0.00108542, gnorm=1.113, clip=0.017, oom=0.000, wall=17953, train_wall=23099
| epoch 008:  18000 / 18876 loss=0.957, copy_alpha=0.690208, ppl=1.94, wps=6510, ups=6, wpb=1031.973, bsz=63.722, num_updates=150133, lr=0.00115926, gnorm=1.112, clip=0.017, oom=0.000, wall=18113, train_wall=23253
| epoch 008 | loss 0.958 | copy_alpha 0.690249 | ppl 1.94 | wps 6508 | ups 6 | wpb 1031.897 | bsz 63.706 | num_updates 151008 | lr 0.00122388 | gnorm 1.111 | clip 0.017 | oom 0.000 | wall 18252 | train_wall 23386
| epoch 008 | valid on 'valid' subset | loss 1.060 | ppl 2.08 | num_updates 151008 | best_loss 1.06 | copy_alpha 0.833001
| epoch 008 | valid on 'valid1' subset | loss 0.490 | ppl 1.40 | num_updates 151008 | best_loss 0.490251 | copy_alpha 0.787127
| epoch 008 | valid on 'valid' subset | loss 1.058 | ppl 2.08 | num_updates 151008 | best_loss 1.05836 | copy_alpha 0.832113
| epoch 008 | valid on 'valid1' subset | loss 0.488 | ppl 1.40 | num_updates 151008 | best_loss 0.488484 | copy_alpha 0.786976
| epoch 009:   1000 / 18876 loss=0.931, copy_alpha=0.696805, ppl=1.91, wps=6452, ups=5, wpb=1018.884, bsz=63.832, num_updates=152009, lr=0.0012978, gnorm=1.112, clip=0.017, oom=0.000, wall=18440, train_wall=23537
| epoch 009:   2000 / 18876 loss=0.922, copy_alpha=0.694194, ppl=1.90, wps=6486, ups=6, wpb=1022.533, bsz=63.772, num_updates=153009, lr=0.00137165, gnorm=1.117, clip=0.018, oom=0.000, wall=18598, train_wall=23688
| epoch 009:   3000 / 18876 loss=0.935, copy_alpha=0.694996, ppl=1.91, wps=6522, ups=6, wpb=1033.304, bsz=63.741, num_updates=154009, lr=0.00144549, gnorm=1.115, clip=0.019, oom=0.000, wall=18758, train_wall=23842
| epoch 009:   4000 / 18876 loss=0.932, copy_alpha=0.694447, ppl=1.91, wps=6518, ups=6, wpb=1030.186, bsz=63.732, num_updates=155009, lr=0.00151934, gnorm=1.116, clip=0.019, oom=0.000, wall=18915, train_wall=23992
| epoch 009:   5000 / 18876 loss=0.932, copy_alpha=0.693756, ppl=1.91, wps=6507, ups=6, wpb=1029.081, bsz=63.736, num_updates=156009, lr=0.00159318, gnorm=1.118, clip=0.019, oom=0.000, wall=19073, train_wall=24144
| epoch 009:   6000 / 18876 loss=0.933, copy_alpha=0.693697, ppl=1.91, wps=6500, ups=6, wpb=1030.343, bsz=63.700, num_updates=157009, lr=0.00166703, gnorm=1.120, clip=0.019, oom=0.000, wall=19233, train_wall=24297
| epoch 009:   7000 / 18876 loss=0.939, copy_alpha=0.693561, ppl=1.92, wps=6493, ups=6, wpb=1028.243, bsz=63.687, num_updates=158009, lr=0.00174088, gnorm=1.122, clip=0.019, oom=0.000, wall=19391, train_wall=24448
| epoch 009:   8000 / 18876 loss=0.943, copy_alpha=0.693697, ppl=1.92, wps=6479, ups=6, wpb=1026.887, bsz=63.701, num_updates=159009, lr=0.00181472, gnorm=1.124, clip=0.019, oom=0.000, wall=19550, train_wall=24601
| epoch 009:   9000 / 18876 loss=0.943, copy_alpha=0.69302, ppl=1.92, wps=6480, ups=6, wpb=1027.599, bsz=63.696, num_updates=160009, lr=0.00188857, gnorm=1.124, clip=0.019, oom=0.000, wall=19710, train_wall=24753
| epoch 009:  10000 / 18876 loss=0.944, copy_alpha=0.692843, ppl=1.92, wps=6480, ups=6, wpb=1027.186, bsz=63.698, num_updates=161009, lr=0.00196242, gnorm=1.125, clip=0.019, oom=0.000, wall=19868, train_wall=24905
| epoch 009:  11000 / 18876 loss=0.948, copy_alpha=0.692717, ppl=1.93, wps=6484, ups=6, wpb=1027.855, bsz=63.692, num_updates=162009, lr=0.00203626, gnorm=1.127, clip=0.020, oom=0.000, wall=20026, train_wall=25057
| epoch 009:  12000 / 18876 loss=0.949, copy_alpha=0.692337, ppl=1.93, wps=6487, ups=6, wpb=1029.599, bsz=63.678, num_updates=163009, lr=0.00211011, gnorm=1.127, clip=0.020, oom=0.000, wall=20187, train_wall=25211
| epoch 009:  13000 / 18876 loss=0.947, copy_alpha=0.69197, ppl=1.93, wps=6481, ups=6, wpb=1029.861, bsz=63.689, num_updates=164009, lr=0.00218395, gnorm=1.127, clip=0.020, oom=0.000, wall=20348, train_wall=25365
| epoch 009:  14000 / 18876 loss=0.948, copy_alpha=0.692011, ppl=1.93, wps=6482, ups=6, wpb=1029.209, bsz=63.697, num_updates=165009, lr=0.0022578, gnorm=1.129, clip=0.019, oom=0.000, wall=20505, train_wall=25516
| epoch 009:  15000 / 18876 loss=0.949, copy_alpha=0.69159, ppl=1.93, wps=6488, ups=6, wpb=1030.115, bsz=63.701, num_updates=166009, lr=0.00233165, gnorm=1.129, clip=0.020, oom=0.000, wall=20664, train_wall=25668
| epoch 009:  16000 / 18876 loss=0.950, copy_alpha=0.691248, ppl=1.93, wps=6492, ups=6, wpb=1030.281, bsz=63.707, num_updates=167009, lr=0.00240549, gnorm=1.129, clip=0.020, oom=0.000, wall=20821, train_wall=25819
| epoch 009:  17000 / 18876 loss=0.950, copy_alpha=0.690867, ppl=1.93, wps=6493, ups=6, wpb=1029.564, bsz=63.713, num_updates=168009, lr=0.00247934, gnorm=1.130, clip=0.020, oom=0.000, wall=20978, train_wall=25969
| epoch 009:  18000 / 18876 loss=0.950, copy_alpha=0.690389, ppl=1.93, wps=6496, ups=6, wpb=1031.057, bsz=63.709, num_updates=169009, lr=0.00255319, gnorm=1.130, clip=0.019, oom=0.000, wall=21139, train_wall=26124
| epoch 009 | loss 0.952 | copy_alpha 0.690288 | ppl 1.93 | wps 6497 | ups 6 | wpb 1031.897 | bsz 63.706 | num_updates 169884 | lr 0.0026178 | gnorm 1.131 | clip 0.020 | oom 0.000 | wall 21280 | train_wall 26259
| epoch 009 | valid on 'valid' subset | loss 1.079 | ppl 2.11 | num_updates 169884 | best_loss 1.06 | copy_alpha 0.829877
| epoch 009 | valid on 'valid1' subset | loss 0.502 | ppl 1.42 | num_updates 169884 | best_loss 0.501859 | copy_alpha 0.788807
| epoch 009 | valid on 'valid' subset | loss 1.056 | ppl 2.08 | num_updates 169884 | best_loss 1.05635 | copy_alpha 0.830009
| epoch 009 | valid on 'valid1' subset | loss 0.487 | ppl 1.40 | num_updates 169884 | best_loss 0.486509 | copy_alpha 0.785119
| done training in 21287.6 seconds
